{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running UnfilteredAI/NSFW-3B on Kaggle GPU\n",
    "\n",
    "This notebook demonstrates how to run the UnfilteredAI/NSFW-3B model on Kaggle's free GPU tier. Based on our research, Kaggle's free tier offers a T4 GPU with approximately 16GB VRAM, which should be sufficient for running the 3B parameter model that requires around 10.6GB VRAM.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, make sure you've enabled GPU acceleration for this notebook:\n",
    "1. Click on the '...' menu in the top-right corner\n",
    "2. Select 'Settings'\n",
    "3. Under 'Accelerator', select 'GPU T4 x1'\n",
    "4. Click 'Save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "Let's install the necessary packages to run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.35.0 torch==2.0.1 sentencepiece==0.1.99 accelerate\n",
    "# Note: We're using 'accelerate' instead of 'accelerators' as it's the correct package name for Hugging Face's acceleration library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Integration Class\n",
    "\n",
    "Let's implement the ModelIntegration class from our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelIntegration:\n",
    "    def __init__(self, model_name: str = \"UnfilteredAI/NSFW-3B\", use_mock: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the model integration.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the Hugging Face model to use. Default is \"UnfilteredAI/NSFW-3B\".\n",
    "            use_mock: If True, will use mock responses instead of loading the model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.mock_mode = use_mock\n",
    "        \n",
    "        # If not in mock mode, try to load the model\n",
    "        if not self.mock_mode:\n",
    "            try:\n",
    "                self._load_model()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "                print(\"Falling back to mock mode\")\n",
    "                self.mock_mode = True\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the LLM model using Hugging Face Transformers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model {self.model_name} from Hugging Face...\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,  # Use half-precision to reduce memory usage\n",
    "                device_map=\"auto\"  # Automatically determine the best device configuration\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully loaded {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    \n",
    "    def generate_story(self, prompt: str, genre: str, length: str, temperature: float = 0.7) -> str:\n",
    "        \"\"\"\n",
    "        Generate a story based on the given parameters.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The story prompt\n",
    "            genre: The genre of the story\n",
    "            length: The desired length (short, medium, long)\n",
    "            temperature: Creativity parameter (0.0 to 1.0)\n",
    "            \n",
    "        Returns:\n",
    "            The generated story text\n",
    "        \"\"\"\n",
    "        if self.mock_mode:\n",
    "            return self._generate_mock_story(prompt, genre, length)\n",
    "        else:\n",
    "            return self._generate_with_model(prompt, genre, length, temperature)\n",
    "    \n",
    "    def _generate_with_model(self, prompt: str, genre: str, length: str, temperature: float) -> str:\n",
    "        \"\"\"\n",
    "        Generate a story using the loaded Hugging Face Transformers model.\n",
    "        \"\"\"\n",
    "        # Map length to approximate token counts\n",
    "        length_to_tokens = {\n",
    "            \"short\": 512,\n",
    "            \"medium\": 1024,\n",
    "            \"long\": 2048\n",
    "        }\n",
    "        max_tokens = length_to_tokens.get(length, 1024)\n",
    "        \n",
    "        # Create a system prompt that guides the model\n",
    "        system_prompt = f\"You are an expert writer of {genre} NSFW stories. \"\n",
    "        system_prompt += f\"Write a {length} story based on the following prompt: {prompt}\\n\\n\"\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(system_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate the story\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove the prompt from the generated text\n",
    "        story = generated_text[len(system_prompt):].strip()\n",
    "        \n",
    "        return story\n",
    "    \n",
    "    def _generate_mock_story(self, prompt: str, genre: str, length: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a mock story for testing purposes.\n",
    "        \"\"\"\n",
    "        stories = {\n",
    "            'romance': f\"The moonlight filtered through the ancient oak trees, casting dancing shadows on the garden path. {prompt} as they found themselves alone in this secluded paradise.\\n\\nTheir hearts raced as they drew closer, the scent of jasmine filling the air. His fingers gently traced her cheek, feeling the softness of her skin under the silver light. She leaned into his touch, her breath catching as his lips found hers.\\n\\nThe kiss deepened, filled with weeks of unspoken desire. Her hands moved to his chest, feeling the steady rhythm of his heartbeat. He pulled her closer, his hands exploring the curve of her back as passion ignited between them.\\n\\nTime seemed to stand still in the moonlit garden, where two souls connected in the most intimate way, their love story unfolding under the watchful eyes of the stars.\",\n",
    "            \n",
    "            'fantasy': f\"In the mystical realm of Aethermoor, {prompt} took place in an enchanted grove where ancient magic flowed through the very air.\\n\\nThe sorceress felt the power surge through her veins as she cast the spell of binding. The warrior's eyes glowed with otherworldly energy as he responded to her magical touch. Runes carved into the ancient stones began to pulse with ethereal light.\\n\\nTheir connection transcended the physical realm, merging their souls in a dance of arcane energy. The magic intensified, wrapping around them like a cocoon of pure desire. As the spell reached its crescendo, reality itself seemed to bend to their will.\\n\\nIn this sacred place where magic and passion intertwined, they became one with the ancient forces that governed their world, their union echoing through the mystical planes for eternity.\",\n",
    "            \n",
    "            \"sci-fi\": f\"Aboard the starship Nebula, {prompt} occurred in the zero-gravity observation deck as they drifted among the stars.\\n\\nThe advanced neural interface pulsed with energy as their minds connected through the quantum link. Her enhanced senses detected his arousal through bio-scans, while his cybernetic implants responded to her pheromones.\\n\\nIn the weightless environment, their movements became a graceful ballet of desire. The holographic displays around them flickered as their passion overloaded the ship's sensors. His synthetic skin, designed to mimic human touch, traced patterns across her genetically enhanced form.\\n\\nAs they reached climax, the artificial gravity briefly malfunctioned, sending waves of pleasure through their enhanced nervous systems. Their union was recorded in the ship's quantum logs as a perfect harmony of human emotion and technological evolution.\",\n",
    "            \n",
    "            'contemporary': f\"In the heart of the city, {prompt} unfolded in a rooftop garden hidden above the bustling streets below.\\n\\nThe sounds of traffic faded into the background as they found their own private world among the potted plants and string lights. Her fingers worked quickly at the buttons of his shirt, revealing the toned chest beneath.\\n\\nHe lifted her effortlessly onto the wrought iron table, her legs wrapping around his waist as their lips met hungrily. The cool metal beneath her back contrasted with the heat of his body pressing against hers.\\n\\nCity lights twinkled around them like distant stars as they explored each other with desperate need. In this secret sanctuary high above the world, they found a moment of pure connection away from the chaos of modern life.\",\n",
    "            \n",
    "            'historical': f\"In the candlelit chambers of the medieval castle, {prompt} transpired during a secret rendezvous between a noble lady and her forbidden lover.\\n\\nThe tapestries on the stone walls seemed to watch as they embraced with the passion of those who knew their time together was fleeting. Her silk gown pooled around her feet as he lifted her onto the canopied bed.\\n\\nThe flickering candlelight cast shadows across his face as he kissed her neck, his hands exploring the curves hidden beneath layers of period clothing. She gasped as his fingers found their way past the intricate laces of her bodice.\\n\\nIn an era where such liaisons could mean death, their love was all the more intense. The heavy wooden door creaked slightly in the night breeze as they lost themselves in each other, knowing that dawn would bring the harsh reality of their separate worlds.\"\n",
    "        }\n",
    "        \n",
    "        story = stories.get(genre, stories['romance'])\n",
    "        \n",
    "        # Adjust length\n",
    "        if length == 'short':\n",
    "            paragraphs = story.split('\\n\\n')\n",
    "            if len(paragraphs) > 2:\n",
    "                story = paragraphs[0] + '\\n\\n' + paragraphs[1]\n",
    "        elif length == 'long':\n",
    "            story += '\\n\\n' + story.replace('their', 'the lovers\'').replace('they', 'the couple')\n",
    "        \n",
    "        return story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "\n",
    "Now let's initialize the model. This will download the model from Hugging Face and load it into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with use_mock=False to use the actual model\n",
    "model = ModelIntegration(model_name=\"UnfilteredAI/NSFW-3B\", use_mock=False)\n",
    "\n",
    "# Check if the model is loaded or in mock mode\n",
    "print(f\"Model is in mock mode: {model.mock_mode}\")\n",
    "if not model.mock_mode:\n",
    "    print(f\"Model loaded successfully: {model.model_name}\")\n",
    "    print(f\"Model device: {model.model.device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.model.parameters()) / 1_000_000:.2f}M\")\n",
    "    print(f\"Model memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Model failed to load, using mock mode instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Story Generation\n",
    "\n",
    "Let's test generating a story with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "prompt = \"Two strangers meet at a masquerade ball\"\n",
    "genre = \"romance\"  # Options: romance, fantasy, sci-fi, contemporary, historical\n",
    "length = \"short\"  # Options: short, medium, long\n",
    "temperature = 0.7  # Controls randomness: 0.0 to 1.0\n",
    "\n",
    "# Measure generation time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the story\n",
    "story = model.generate_story(prompt, genre, length, temperature)\n",
    "\n",
    "# Calculate generation time\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(\"\\nGenerated Story:\\n\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple UI for Testing\n",
    "\n",
    "Let's create a simple UI using IPython widgets to test different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create input widgets\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='Two strangers meet at a masquerade ball',\n",
    "    placeholder='Enter your story prompt here...',\n",
    "    description='Prompt:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "genre_dropdown = widgets.Dropdown(\n",
    "    options=['romance', 'fantasy', 'sci-fi', 'contemporary', 'historical'],\n",
    "    value='romance',\n",
    "    description='Genre:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "length_dropdown = widgets.Dropdown(\n",
    "    options=['short', 'medium', 'long'],\n",
    "    value='short',\n",
    "    description='Length:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Story',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Click to generate a story',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output(layout=widgets.Layout(border='1px solid black', width='100%', height='300px', overflow='auto'))\n",
    "\n",
    "# Define button click handler\n",
    "def on_generate_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(\"Generating story... Please wait.\")\n",
    "        \n",
    "        # Get input values\n",
    "        prompt = prompt_input.value\n",
    "        genre = genre_dropdown.value\n",
    "        length = length_dropdown.value\n",
    "        temperature = temperature_slider.value\n",
    "        \n",
    "        # Measure generation time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate the story\n",
    "        story = model.generate_story(prompt, genre, length, temperature)\n",
    "        \n",
    "        # Calculate generation time\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        clear_output()\n",
    "        print(f\"Generation time: {generation_time:.2f} seconds\\n\")\n",
    "        print(\"Generated Story:\\n\")\n",
    "        print(story)\n",
    "\n",
    "# Connect the button click event to the handler\n",
    "generate_button.on_click(on_generate_button_clicked)\n",
    "\n",
    "# Display the UI\n",
    "display(prompt_input, genre_dropdown, length_dropdown, temperature_slider, generate_button, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "\n",
    "If you're experiencing memory issues, you can try the following techniques to reduce memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current GPU memory usage\n",
    "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Max GPU memory usage: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Free up memory if needed\n",
    "def free_memory():\n",
    "    if hasattr(model, 'model') and model.model is not None:\n",
    "        # Move model to CPU\n",
    "        model.model.to('cpu')\n",
    "        # Clear CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Model moved to CPU and CUDA cache cleared\")\n",
    "        print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No model loaded in GPU memory\")\n",
    "\n",
    "# Uncomment the line below if you need to free up memory\n",
    "# free_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to run the UnfilteredAI/NSFW-3B model on Kaggle's GPU. The model requires approximately 10.6GB of VRAM, which should fit within Kaggle's free GPU tier (T4 with ~16GB VRAM).\n",
    "\n",
    "Key points:\n",
    "\n",
    "1. The model is loaded using Hugging Face Transformers with half-precision (float16) to reduce memory usage\n",
    "2. We've implemented the same ModelIntegration class used in the main application\n",
    "3. The interactive UI allows for testing different parameters\n",
    "4. Memory management techniques are provided if needed\n",
    "\n",
    "Remember that Kaggle has a 30-hour per week limit for GPU usage on the free tier, so use your GPU time wisely!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 }
}