{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSFW-3B Model API on Kaggle\n",
    "\n",
    "This notebook demonstrates how to run the UnfilteredAI/NSFW-3B model on Kaggle's GPU and expose it as an API endpoint that can be called from other applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, make sure you've enabled GPU acceleration and internet access for this notebook:\n",
    "1. Click on the '...' menu in the top-right corner\n",
    "2. Select 'Settings'\n",
    "3. Under 'Accelerator', select 'GPU T4 x1'\n",
    "4. Enable 'Internet' under the Internet section\n",
    "5. Click 'Save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.35.0 torch==2.0.1 sentencepiece==0.1.99 accelerate flask flask-cors pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Integration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelIntegration:\n",
    "    def __init__(self, model_name: str = \"UnfilteredAI/NSFW-3B\", use_mock: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the model integration.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the Hugging Face model to use. Default is \"UnfilteredAI/NSFW-3B\".\n",
    "            use_mock: If True, will use mock responses instead of loading the model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.mock_mode = use_mock\n",
    "        \n",
    "        # If not in mock mode, try to load the model\n",
    "        if not self.mock_mode:\n",
    "            try:\n",
    "                self._load_model()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "                print(\"Falling back to mock mode\")\n",
    "                self.mock_mode = True\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the LLM model using Hugging Face Transformers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model {self.model_name} from Hugging Face...\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,  # Use half-precision to reduce memory usage\n",
    "                device_map=\"auto\"  # Automatically determine the best device configuration\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully loaded {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    \n",
    "    def generate_story(self, prompt: str, genre: str, length: str, temperature: float = 0.7) -> str:\n",
    "        \"\"\"\n",
    "        Generate a story based on the given parameters.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The story prompt\n",
    "            genre: The genre of the story\n",
    "            length: The desired length (short, medium, long)\n",
    "            temperature: Creativity parameter (0.0 to 1.0)\n",
    "            \n",
    "        Returns:\n",
    "            The generated story text\n",
    "        \"\"\"\n",
    "        if self.mock_mode:\n",
    "            return self._generate_mock_story(prompt, genre, length)\n",
    "        else:\n",
    "            return self._generate_with_model(prompt, genre, length, temperature)\n",
    "    \n",
    "    def _generate_with_model(self, prompt: str, genre: str, length: str, temperature: float) -> str:\n",
    "        \"\"\"\n",
    "        Generate a story using the loaded Hugging Face Transformers model.\n",
    "        \"\"\"\n",
    "        # Map length to approximate token counts\n",
    "        length_to_tokens = {\n",
    "            \"short\": 512,\n",
    "            \"medium\": 1024,\n",
    "            \"long\": 2048\n",
    "        }\n",
    "        max_tokens = length_to_tokens.get(length, 1024)\n",
    "        \n",
    "        # Create a system prompt that guides the model\n",
    "        system_prompt = f\"You are an expert writer of {genre} NSFW stories. \"\n",
    "        system_prompt += f\"Write a {length} story based on the following prompt: {prompt}\\n\\n\"\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(system_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate the story\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove the prompt from the generated text\n",
    "        story = generated_text[len(system_prompt):].strip()\n",
    "        \n",
    "        return story\n",
    "    \n",
    "    def _generate_mock_story(self, prompt: str, genre: str, length: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a mock story for testing purposes.\n",
    "        \"\"\"\n",
    "        stories = {\n",
    "            'romance': f\"The moonlight filtered through the ancient oak trees, casting dancing shadows on the garden path. {prompt} as they found themselves alone in this secluded paradise.\\n\\nTheir hearts raced as they drew closer, the scent of jasmine filling the air. His fingers gently traced her cheek, feeling the softness of her skin under the silver light. She leaned into his touch, her breath catching as his lips found hers.\\n\\nThe kiss deepened, filled with weeks of unspoken desire. Her hands moved to his chest, feeling the steady rhythm of his heartbeat. He pulled her closer, his hands exploring the curve of her back as passion ignited between them.\\n\\nTime seemed to stand still in the moonlit garden, where two souls connected in the most intimate way, their love story unfolding under the watchful eyes of the stars.\",\n",
    "            \n",
    "            'fantasy': f\"In the mystical realm of Aethermoor, {prompt} took place in an enchanted grove where ancient magic flowed through the very air.\\n\\nThe sorceress felt the power surge through her veins as she cast the spell of binding. The warrior's eyes glowed with otherworldly energy as he responded to her magical touch. Runes carved into the ancient stones began to pulse with ethereal light.\\n\\nTheir connection transcended the physical realm, merging their souls in a dance of arcane energy. The magic intensified, wrapping around them like a cocoon of pure desire. As the spell reached its crescendo, reality itself seemed to bend to their will.\\n\\nIn this sacred place where magic and passion intertwined, they became one with the ancient forces that governed their world, their union echoing through the mystical planes for eternity.\",\n",
    "            \n",
    "            \"sci-fi\": f\"Aboard the starship Nebula, {prompt} occurred in the zero-gravity observation deck as they drifted among the stars.\\n\\nThe advanced neural interface pulsed with energy as their minds connected through the quantum link. Her enhanced senses detected his arousal through bio-scans, while his cybernetic implants responded to her pheromones.\\n\\nIn the weightless environment, their movements became a graceful ballet of desire. The holographic displays around them flickered as their passion overloaded the ship's sensors. His synthetic skin, designed to mimic human touch, traced patterns across her genetically enhanced form.\\n\\nAs they reached climax, the artificial gravity briefly malfunctioned, sending waves of pleasure through their enhanced nervous systems. Their union was recorded in the ship's quantum logs as a perfect harmony of human emotion and technological evolution.\",\n",
    "            \n",
    "            'contemporary': f\"In the heart of the city, {prompt} unfolded in a rooftop garden hidden above the bustling streets below.\\n\\nThe sounds of traffic faded into the background as they found their own private world among the potted plants and string lights. Her fingers worked quickly at the buttons of his shirt, revealing the toned chest beneath.\\n\\nHe lifted her effortlessly onto the wrought iron table, her legs wrapping around his waist as their lips met hungrily. The cool metal beneath her back contrasted with the heat of his body pressing against hers.\\n\\nCity lights twinkled around them like distant stars as they explored each other with desperate need. In this secret sanctuary high above the world, they found a moment of pure connection away from the chaos of modern life.\",\n",
    "            \n",
    "            'historical': f\"In the candlelit chambers of the medieval castle, {prompt} transpired during a secret rendezvous between a noble lady and her forbidden lover.\\n\\nThe tapestries on the stone walls seemed to watch as they embraced with the passion of those who knew their time together was fleeting. Her silk gown pooled around her feet as he lifted her onto the canopied bed.\\n\\nThe flickering candlelight cast shadows across his face as he kissed her neck, his hands exploring the curves hidden beneath layers of period clothing. She gasped as his fingers found their way past the intricate laces of her bodice.\\n\\nIn an era where such liaisons could mean death, their love was all the more intense. The heavy wooden door creaked slightly in the night breeze as they lost themselves in each other, knowing that dawn would bring the harsh reality of their separate worlds.\"\n",
    "        }\n",
    "        \n",
    "        story = stories.get(genre, stories['romance'])\n",
    "        \n",
    "        # Adjust length\n",
    "        if length == 'short':\n",
    "            paragraphs = story.split('\\n\\n')\n",
    "            if len(paragraphs) > 2:\n",
    "                story = paragraphs[0] + '\\n\\n' + paragraphs[1]\n",
    "        elif length == 'long':\n",
    "            story += '\\n\\n' + story.replace('their', 'the lovers\'').replace('they', 'the couple')\n",
    "        \n",
    "        return story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "\n",
    "Now let's initialize the model. This will download the model from Hugging Face and load it into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with use_mock=False to use the actual model\n",
    "model = ModelIntegration(model_name=\"UnfilteredAI/NSFW-3B\", use_mock=False)\n",
    "\n",
    "# Check if the model is loaded or in mock mode\n",
    "print(f\"Model is in mock mode: {model.mock_mode}\")\n",
    "if not model.mock_mode:\n",
    "    print(f\"Model loaded successfully: {model.model_name}\")\n",
    "    print(f\"Model device: {model.model.device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.model.parameters()) / 1_000_000:.2f}M\")\n",
    "    print(f\"Model memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Model failed to load, using mock mode instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flask API\n",
    "\n",
    "Now let's create a Flask API to expose the model as a service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for all routes\n",
    "\n",
    "# API endpoint for generating stories\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "def generate_story():\n",
    "    data = request.json\n",
    "    prompt = data.get('prompt', '')\n",
    "    genre = data.get('genre', 'romance')\n",
    "    length = data.get('length', 'medium')\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Measure generation time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate the story using the model integration\n",
    "    story = model.generate_story(prompt, genre, length, temperature)\n",
    "    \n",
    "    # Calculate generation time\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    return jsonify({\n",
    "        'story': story,\n",
    "        'status': 'success',\n",
    "        'generation_time': generation_time\n",
    "    })\n",
    "\n",
    "# Health check endpoint\n",
    "@app.route('/health')\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model_loaded': not model.mock_mode,\n",
    "        'model_name': model.model_name,\n",
    "        'memory_usage_gb': torch.cuda.memory_allocated() / 1024**3 if not model.mock_mode else 0\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expose API with ngrok\n",
    "\n",
    "Now let's expose our API using ngrok so it can be accessed from outside Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ngrok\n",
    "# Note: You'll need to sign up for a free ngrok account and get your authtoken\n",
    "# Replace YOUR_AUTHTOKEN with your actual ngrok authtoken\n",
    "# ngrok.set_auth_token(\"YOUR_AUTHTOKEN\")\n",
    "\n",
    "# Start ngrok tunnel\n",
    "ngrok_tunnel = ngrok.connect(5000)\n",
    "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
    "\n",
    "# Function to run Flask app in a separate thread\n",
    "def run_flask_app():\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start Flask app in a separate thread\n",
    "flask_thread = threading.Thread(target=run_flask_app)\n",
    "flask_thread.daemon = True  # This ensures the thread will be killed when the notebook is stopped\n",
    "flask_thread.start()\n",
    "\n",
    "print(\"API server is running. You can now access it at the public URL above.\")\n",
    "print(\"Health check endpoint: {}/health\".format(ngrok_tunnel.public_url))\n",
    "print(\"Generate story endpoint: {}/api/generate\".format(ngrok_tunnel.public_url))\n",
    "print(\"\\nKeep this cell running to maintain the API server.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the API\n",
    "\n",
    "Let's test our API with a sample request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your actual ngrok URL\n",
    "api_url = ngrok_tunnel.public_url + \"/api/generate\"\n",
    "\n",
    "# Test parameters\n",
    "payload = {\n",
    "    \"prompt\": \"Two strangers meet at a masquerade ball\",\n",
    "    \"genre\": \"romance\",\n",
    "    \"length\": \"short\",\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Send request to our API\n",
    "response = requests.post(api_url, json=payload)\n",
    "\n",
    "# Print response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"Generation time: {result.get('generation_time', 0):.2f} seconds\")\n",
    "    print(\"\\nGenerated Story:\\n\")\n",
    "    print(result.get('story', 'No story generated'))\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Connect to This API\n",
    "\n",
    "You can now connect to this API from any application, including your GitHub Codespaces app. Here's how to modify your GitHub Codespaces app to use this Kaggle-hosted model API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# In your GitHub Codespaces app.py file, modify the ModelIntegration class to use the Kaggle API\n",
    "\n",
    "import requests\n",
    "\n",
    "class KaggleModelConnector:\n",
    "    def __init__(self, api_url):\n",
    "        self.api_url = api_url\n",
    "        self.mock_mode = False\n",
    "        \n",
    "    def generate_story(self, prompt, genre, length, temperature):\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"genre\": genre,\n",
    "            \"length\": length,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            return result.get('story', 'Error generating story')\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to Kaggle API: {e}\")\n",
    "            # Fall back to mock mode if API is unavailable\n",
    "            self.mock_mode = True\n",
    "            return \"API connection error. Falling back to mock mode.\"\n",
    "\n",
    "# Replace the model initialization in app.py\n",
    "# model = ModelIntegration(model_name=\"UnfilteredAI/NSFW-3B\", use_mock=True)\n",
    "\n",
    "# With this:\n",
    "kaggle_api_url = \"https://YOUR-NGROK-URL.ngrok.io/api/generate\"  # Replace with your actual ngrok URL\n",
    "model = KaggleModelConnector(kaggle_api_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Keep this notebook running**: The API will only be available as long as this notebook is running\n",
    "2. **ngrok URL expiration**: The ngrok URL will change each time you run the notebook\n",
    "3. **Kaggle time limits**: Remember that Kaggle has a 30-hour per week limit for GPU usage\n",
    "4. **Memory management**: Monitor GPU memory usage to avoid out-of-memory errors\n",
    "\n",
    "This approach allows you to leverage Kaggle's free GPU resources for model inference while keeping your application's frontend and other components in GitHub Codespaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 }
}